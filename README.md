# Telecom Customer Churn Prediction  
_Logistic Regression â€¢ Random Forest â€¢ XGBoost â€¢ Optuna â€¢ Stacking Ensemble â€¢ Streamlit App_

This project builds a complete Machine Learning system to predict **customer churn** for a telecom company.  
It includes **EDA â†’ preprocessing â†’ imbalance handling â†’ model training â†’ hyperparameter tuning â†’ ensemble learning â†’ evaluation â†’ deployment via Streamlit**.

---

## ðŸ“Œ 1. Dataset Details (telecom_churn.csv)

### What is churn?
A customer is considered "churned" when they stop using the service.  
Goal: Predict whether a customer will churn (`1`) or stay (`0`).

### Dataset Size
- **Rows:** 3333  
- **Columns:** 11  

### Target Column: `Churn`
- `0` â†’ Not churned  
- `1` â†’ Churned  

### Class Imbalance
- Churn = 0 â†’ 2850  
- Churn = 1 â†’ 483  
â®• Only **14.5% churners** â†’ dataset is **imbalanced**.

---

## ðŸ“Œ 2. Why Logistic Regression (Not Linear Regression)
Logistic Regression models the probability:

\[
P(y=1|x) = \frac{1}{1 + e^{-(w^Tx + b)}}
\]

It is ideal for **binary classification**, unlike Linear Regression which predicts continuous values.

---

## ðŸ“Œ 3. ML Workflow (Pipeline)

### **Step 1 â€” Data Loading**
Handled by `data_loading.py`  
Ensures correct columns and clean structure.

---

### **Step 2 â€” Preprocessing**
Performed in `preprocessing.py`:
- Train/Test split (stratified)
- Scaling using `StandardScaler`
- Outlier & skewness checks

Scaling is crucial for algorithms like Logistic Regression.

---

### **Step 3 â€” Exploratory Data Analysis (EDA)**
In `eda.py`, we analyze:
- Feature distributions  
- Correlations  
- Outliers  
- Customer behavior patterns  

---

### **Step 4 â€” Handling Class Imbalance (SMOTE)**  
Why SMOTE?
- Prevents model from being biased toward majority class  
- Generates synthetic minority samples  
- Improves recall for churners  

---

## ðŸ“Œ 4. Ensemble Learning (Very Important)

To improve model robustness, we trained multiple models:

### **Base Models**
- Logistic Regression  
- Random Forest  
- XGBoost  

### **Why Ensemble?**
Each model captures different patterns:
- Logistic Regression â†’ linear structure  
- Random Forest â†’ non-linear interactions  
- XGBoost â†’ gradient-boosted precision  

### **Voting Classifier**
- Combines predictions by averaging (soft vote)  
Useful to reduce variance.

### **Stacking Ensemble (Best Model)**
Uses:
- Level-0 models: `Logistic Regression + Random Forest + XGBoost`
- Meta-model: `Logistic Regression`

The meta-model **learns how much to trust each base model**, giving the stacking model superior performance.

---

## ðŸ“Œ 5. Hyperparameter Tuning (Optuna)

Optuna was used for:
- Searching best `max_depth`, `learning_rate`, `n_estimators`
- Pruning bad trials early  
- Achieving better performance than grid search  

### **Final Optuna-tuned XGBoost Results**

RMSE = 0.2873
MAE = 0.1380
R2 = 0.3360


---

## ðŸ“Œ 6. Baseline Model Performance

Logistic Regression: RMSE=0.3884, MAE=0.2926, R2=-0.2139
Random Forest: RMSE=0.2929, MAE=0.1759, R2=0.3099
XGBoost: RMSE=0.2982, MAE=0.1251, R2=0.2846
Lasso: RMSE=0.5000
Ridge: RMSE=0.4054


---

## ðŸ“Œ 7. Model Comparison (Visual Outputs)

### **Predictions vs Actuals**
![Prediction Comparison](images/All%20Model%20Prediction.png)

---

### **RMSE Comparison**
![RMSE Comparison](images/All%20Model%20Losses.png)

---

### **Confusion Matrix & Metrics**
![Confusion Matrix](images/Matrices.png)

---

## ðŸ“Œ 8. Streamlit App (Deployment Preview)

A simple UI for entering customer features and getting a churn prediction.

![Streamlit UI](images/Predict1.png)

---

## ðŸ“Œ 9. Project Structure
HCL/
â”œâ”€â”€ data_loading.py
â”œâ”€â”€ preprocessing.py
â”œâ”€â”€ eda.py
â”œâ”€â”€ training.py
â”œâ”€â”€ evaluation.py
â”œâ”€â”€ app.py
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ All_Model_Prediction.png
â”‚ â”œâ”€â”€ All_Model_Losses.png
â”‚ â”œâ”€â”€ Matrices.png
â”‚ â”œâ”€â”€ Predict1.png
â”œâ”€â”€ FINALHackathon_HCLTECH.ipynb
â””â”€â”€ requirements.txt


---

## ðŸ“Œ 10. Open Notebook in Google Colab

Click below to view the full notebook:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Soumensun/HCL/blob/main/FINALHackathon_HCLTECH.ipynb)

---

## ðŸ“Œ 11. Summary

âœ” Addressed data imbalance with SMOTE  
âœ” Performed comprehensive EDA  
âœ” Tuned models using Optuna  
âœ” Compared Logistic Regression, RF, XGB, Stacking  
âœ” Best performance from **Stacking Ensemble**  
âœ” Deployed model using **Streamlit**  

This project demonstrates expertise in:
- Machine Learning  
- Feature engineering  
- Imbalanced learning  
- Hyperparameter tuning  
- Ensemble methods  
- Real-world deployment  

---

